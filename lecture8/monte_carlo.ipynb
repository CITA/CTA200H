{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods: Numerical computations through random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Monte Carlo?\n",
    "\n",
    "### Because some problems are too complicated or too uncertain to solve exactly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw random samples that follow a predefined probability distribution to:\n",
    "\n",
    "* find (multiple) maxima of the probability distribution\n",
    "* calculate integrals (i.e., expectation values) of functions, weighted by the probability distribution\n",
    "* characterize the probability distribution, find correlations between parameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib \n",
    "% matplotlib inline\n",
    "% config InlineBackend.figure_format='retina'\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "matplotlib.rcParams['xtick.labelsize'] = 14.0\n",
    "matplotlib.rcParams['ytick.labelsize'] = 14.0\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100000 # the more random samples we have, the more accurate our result is\n",
    "x = np.random.uniform(-1.,1.,N)\n",
    "y = np.random.uniform(-1.,1.,N)\n",
    "r=np.sqrt(x*x+y*y)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(x,y,'x')\n",
    "plt.plot(x[r<1],y[r<1],'x')\n",
    "plt.show()\n",
    "print '2D:',4.*len(x[r<1])/len(x)\n",
    "\n",
    "z = np.random.uniform(-1.,1.,N)\n",
    "r=np.sqrt(x*x+y*y+z*z)\n",
    "print '3D:',6.*len(x[r<1])/len(x)\n",
    "\n",
    "t = np.random.uniform(-1.,1.,N)\n",
    "r=np.sqrt(x*x+y*y+z*z+t*t)\n",
    "print '4D:',np.sqrt(32.*len(x[r<1])/len(x))\n",
    "\n",
    "f = np.random.uniform(-1.,1.,N)\n",
    "r=np.sqrt(x*x+y*y+z*z+t*t+f*f)\n",
    "print '5D:',np.sqrt(60.*len(x[r<1])/len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing samples from a one-dimensional PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing uniformly distributed (pseudo-)random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers are deterministic. However, we can generate so-called pseudo-random numbers (or even trully random ones, but that's difficult and slow).\n",
    "\n",
    "Idea: Use some formula whose outcome depends sensitively on the input; iterate.\n",
    "\n",
    "Note: Need starting point for the iteration (random number seed), but that makes the result reproducible (good for testing and debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rng(m=2**32, a=1103515245, b=12345):\n",
    "    \"\"\"This function updates the (pseudo-)random number to a new one.\"\"\"\n",
    "    rng.current = (a*rng.current + b) % m\n",
    "    return 1.*rng.current/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.current = 101 #setting the random number seed\n",
    "random_numbers = np.array([rng() for i in range(1000)]) #Draw a few random numbers\n",
    "plt.hist(random_numbers,range=(0.,1.),bins=10) #plot them\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, this is already implemented as np.random.random() (or np.random.uniform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "random_numbers2 = np.random.random(1000)\n",
    "plt.hist(random_numbers2,range=(0.,1.),bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None)\n",
    "print np.random.random(1)\n",
    "np.random.seed(10)\n",
    "print np.random.random(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing non-uniformly distributed random values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw random numbers uniformly between 0 and 1 and then transform them to follow any one-dimensional distribution. In this example, we will use a simple Gaussian distribution.\n",
    "\n",
    "* Calculate cumulative probability distribution (CDF): $\\mathrm{CDF}(x) = \\int_{-\\infty}^x \\mathrm{d}x' P(x)$\n",
    "* invert that function to get the inverse cumulative probability distribution (iCDF)\n",
    "* draw random values $y$ between 0 and 1\n",
    "* evaluate $\\mathrm{iCDF}(y)$ to find values of $x$ following the distribution $P(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -10. #set up an array of x-values\n",
    "xmax = 10.\n",
    "dx = 0.01\n",
    "x = np.arange(xmin,xmax + dx,dx)\n",
    "\n",
    "sigma = 2. #set the standard deviation of the Gaussian\n",
    "P = 1./(2.*np.pi*sigma**2)**0.5*np.exp(-x**2/(2.*sigma**2)) #Gaussian PDF\n",
    "\n",
    "plt.plot(x,P)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$P(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "CDF = 0.5*(1. + np.sign(x)*erf(np.abs(x/2**0.5/sigma))) #Gaussian CDF\n",
    "\n",
    "plt.plot(x,CDF)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'CDF$(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfinv\n",
    "y = np.arange(0.,1.+0.01,0.01)\n",
    "\n",
    "def iCDF(y,sigma): #inverse CDF\n",
    "    return 2**0.5*sigma*erfinv(2.*y - 1.)\n",
    "\n",
    "iCDF_arr = iCDF(y,sigma)\n",
    "plt.plot(y,iCDF_arr)\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000 #Number of random samples to draw\n",
    "\n",
    "yvals = np.random.uniform(0.,1.,n) #Draw uniformly distributed values for y\n",
    "xvals = iCDF(yvals,sigma) #Convert these values to values of x\n",
    "\n",
    "plt.hist(xvals,range=(-10,10),bins=20,normed=True) #plot a histogram for x\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'sample distribution')\n",
    "plt.plot(x,P,label='exact PDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these random samples, we can estimate some expectation values, for example the first few moments of the Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'zeroth order:', 1., (xvals**0).sum()/n\n",
    "print 'first order:', 0., (xvals**1).sum()/n\n",
    "print 'second order:', sigma**2, (xvals**2).sum()/n\n",
    "print 'third order:', 0., (xvals**3).sum()/n\n",
    "print 'fourth order:', 3.*sigma**4, (xvals**4).sum()/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy, drawing Gaussian random numbers is also provided as a predefined function (np.random.normal()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals2 = np.random.normal(0.,sigma,n)\n",
    "\n",
    "plt.hist(xvals2,range=(-10,10),bins=20,normed=True) #plot a histogram for x\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'sample distribution')\n",
    "plt.plot(x,P,label='exact PDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing from N-dimensional PDFs -- rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more than one dimension, there is no well-defined cumulative distribution function. Thus, we have to come up with a new idea:\n",
    "\n",
    "* Sample (some region of) N-dimensional space following a simple PDF $Q(x)$ (we will use a uniform distribution).\n",
    "* Add an acceptance-rejection step that accepts the sample $x$ with probability $P(x)/(c\\ Q(x))$.\n",
    "* Choose $c$ such that $P(x) < c\\ Q(x)$ for all $x$ (but ideally not too large).\n",
    "\n",
    "We will use a two-dimensional Gaussian in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a two-dimensional grid for plotting purposes:\n",
    "xvals = np.arange(-10,10.01,0.01)\n",
    "yvals = np.arange(-10,10.01,0.01)\n",
    "gridx, gridy = np.meshgrid(xvals,yvals)\n",
    "\n",
    "#Define the two-dimensional Gaussian:\n",
    "cov_xx = 6.\n",
    "cov_yy = 1.5\n",
    "cov_xy = 1.0\n",
    "cov = np.array([[cov_xx,cov_xy],[cov_xy,cov_yy]])\n",
    "invcov = np.linalg.inv(cov)\n",
    "\n",
    "def twoDGauss(x,y,cov,invcov):\n",
    "    det = np.linalg.det(cov)\n",
    "    norm = 1./(2.*np.pi)/det**0.5\n",
    "    return norm*np.exp(-0.5*(x*(invcov[0,0]*x + invcov[0,1]*y) + y*(invcov[1,0]*x + invcov[1,1]*y)))\n",
    "\n",
    "#Evaluate the Gaussian at all grid points and plot it:\n",
    "G = twoDGauss(gridx,gridy,cov,invcov)\n",
    "\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define drawing step:\n",
    "def draw_sample():\n",
    "    return np.random.uniform(-10,10,2) #two-dimensional uniform distribution\n",
    "\n",
    "#Define the acceptance-rejection step:\n",
    "c = 40.\n",
    "D = 2 #dimension of the parameter space\n",
    "\n",
    "def AcceptReject(c,D,P,*Pargs):\n",
    "    \"\"\"This function returns `True' if the sample is accepted and `False' if not. We use a\n",
    "    variable-length argument list `*Pargs' to be able to use any probability function `P' that we\n",
    "    might come up with\"\"\"\n",
    "    Pval = P(*Pargs)\n",
    "    proposalval = 1./20**D #The proposal density is 1/20*1/20\n",
    "    prob = Pval/(c*proposalval)\n",
    "    return np.random.choice([True,False],p=[prob,1.-prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw some samples:\n",
    "N = 10000 #number of samples to draw\n",
    "count = 0 #count the accepted samples\n",
    "samples = [] #store the accepted samples\n",
    "\n",
    "for i in range(N):\n",
    "    samp = draw_sample()\n",
    "    acc = AcceptReject(c,D,twoDGauss,samp[0],samp[1],cov,invcov)\n",
    "    if acc:\n",
    "        count += 1\n",
    "        samples.append(samp)\n",
    "samples = np.array(samples)\n",
    "\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.plot(samples[:,0],samples[:,1],'.',color='red')\n",
    "plt.show()\n",
    "print 'acceptance ratio:', 1.*count/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate some moments of the PDF:\n",
    "samples = np.array(samples)\n",
    "print 'mean x:', np.mean(samples[:,0])\n",
    "print 'mean y:', np.mean(samples[:,1])\n",
    "print 'variance in x-direction:', np.mean(samples[:,0]**2)\n",
    "print 'variance in y-direction:', np.mean(samples[:,1]**2)\n",
    "print 'covariance of x and y:', np.mean(samples[:,0]*samples[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't know we were drawing from a Gaussian distribution, this would be invaluable information in order to characterize our PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we go to higher dimensions. For simplicity, we use a symmetric Gaussian without correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the D-dimensional Gaussian:\n",
    "sigma = 2.\n",
    "def DdimGauss(xvec,sigma,D):\n",
    "    det = sigma**(2*D)\n",
    "    norm = 1./(2.*np.pi)**(D/2.)/det**0.5\n",
    "    return norm*np.exp(-0.5*np.dot(xvec,xvec/sigma**2))\n",
    "\n",
    "#Define the drawing step:\n",
    "def draw_sample_Ddim(D):\n",
    "    return np.random.uniform(-10,10,D) #D-dimensional uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Draw some samples:\n",
    "N = 10000 #number of samples to draw\n",
    "count = 0 #count the accepted samples\n",
    "samples = [] #store the accepted samples\n",
    "D = 2\n",
    "\n",
    "#Here we are cheating a bit: Since we know what the maximum of the D-dimensional PDF is, we can\n",
    "#calculate the optimal value of c to use:\n",
    "c = DdimGauss(np.zeros(D),sigma,D)*20.**D\n",
    "\n",
    "for i in range(N):\n",
    "    samp = draw_sample_Ddim(D)\n",
    "    acc = AcceptReject(c,D,DdimGauss,samp,sigma,D)\n",
    "    if acc:\n",
    "        count += 1\n",
    "        samples.append(samp)\n",
    "\n",
    "print 'acceptance ratio:', 1.*count/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is known as \"the curse of dimensionality\". What do you think we could do to increase the acceptance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have been drawing samples randomly in some range without using any 'past experience' of where samples have been accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo -- Metropolis Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain: Draw random samples in an ordered chain:\n",
    "\n",
    "* \"Markov\" property: Each sample depends on the sample before, but not on any other samples\n",
    "* Pro: Curse of dimensionality less severe\n",
    "* Con: Successive samples are correlated -> need to discard many of them to obtain a set of independent samples\n",
    "\n",
    "Specific example: Metropolis-Hastings\n",
    "\n",
    "* Select a \"proposal density\" $Q(x'|x)$ to draw a new sample $x'$ given a previous sample $x$\n",
    "* If $P(x') \\geq P(x)$, accept the new sample\n",
    "* If $P(x') < P(x)$, accept the new sample with probability $(P(x')/P(x))(Q(x|x')/Q(x'|x))$\n",
    "* If a sample is rejected, the chain stays at the same position (i.e., the old sample is repeated).\n",
    "* Note that the normalization of $P$ doesn't matter.\n",
    "\n",
    "Let's work again with our two-dimensional Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a two-dimensional grid for plotting purposes:\n",
    "xvals = np.arange(-10,10.01,0.01)\n",
    "yvals = np.arange(-10,10.01,0.01)\n",
    "gridx, gridy = np.meshgrid(xvals,yvals)\n",
    "\n",
    "#Define the two-dimensional Gaussian:\n",
    "cov_xx = 6.\n",
    "cov_yy = 1.5\n",
    "cov_xy = 1.0\n",
    "cov = np.array([[cov_xx,cov_xy],[cov_xy,cov_yy]])\n",
    "invcov = np.linalg.inv(cov)\n",
    "def twoDGauss(samp,cov,invcov):\n",
    "    x = samp[0]\n",
    "    y = samp[1]\n",
    "    det = np.linalg.det(cov)\n",
    "    norm = 1./(2.*np.pi)/det**0.5\n",
    "    return norm*np.exp(-0.5*(x*(invcov[0,0]*x + invcov[0,1]*y) + y*(invcov[1,0]*x + invcov[1,1]*y)))\n",
    "\n",
    "#Evaluate the Gaussian at all grid points and plot it:\n",
    "G = twoDGauss([gridx,gridy],cov,invcov)\n",
    "\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the proposal step (we are using a symmetric Gaussian proposal density):\n",
    "def proposal(oldsamp,sigmaprop,D):\n",
    "    newsamp = oldsamp + np.random.normal(0.,sigmaprop,D)\n",
    "    return newsamp\n",
    "\n",
    "#Define the acceptance-rejection step (return the new sample and a boolean that tells us whether\n",
    "#or not the new sample was accepted):\n",
    "def accept(newsamp,oldsamp,paramrange,P,*Pargs):\n",
    "    if not ((np.array([p1 - p2 for p1, p2 in zip(newsamp, np.transpose(paramrange)[:][0])])>0).all() \\\n",
    "            and (np.array([p1 - p2 for p1, p2 in zip(np.transpose(paramrange)[:][1],newsamp)])>0).all()):\n",
    "        acc = False\n",
    "        return acc, oldsamp # make sure the samples are in the desired range\n",
    "    newprob = P(newsamp,*Pargs)\n",
    "    oldprob = P(oldsamp,*Pargs)\n",
    "    if newprob >= oldprob:\n",
    "        acc = True\n",
    "        return acc, newsamp\n",
    "    else:\n",
    "        prob = newprob/oldprob\n",
    "        acc = np.random.choice([True,False],p=[prob,1.-prob])\n",
    "        return acc, acc*newsamp + (1. - acc)*oldsamp #Note that this is either newsamp or oldsamp\n",
    "\n",
    "#Define function that runs an entire chain:\n",
    "def run_chain(steps,paramrange,sigmaprop,D,P,*Pargs):\n",
    "    oldsamp=np.array([np.random.uniform(paramrange[d][0],paramrange[d][1]) for d in range(D)])#Draw a random starting point\n",
    "    count = 0 #Count the number of accepted samples\n",
    "    samples = [oldsamp] #Store all samples\n",
    "    for i in range(steps):\n",
    "        newsamp = proposal(oldsamp,sigmaprop,D) #Propose a new sample\n",
    "        acc, newsamp = accept(newsamp,oldsamp,paramrange,P,*Pargs) #decide whether or not to accept it\n",
    "        samples.append(newsamp) #Add the sample to the list of samples\n",
    "        if acc:\n",
    "            count += 1\n",
    "        oldsamp = newsamp #Move to the new sample\n",
    "    ar = 1.*count/steps #compute the acceptance ratio\n",
    "    return np.array(samples), ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a chain:\n",
    "np.random.seed(11)\n",
    "Nsteps = 200 #number of steps to run the chain for\n",
    "sigmaprop = 1 #width of the proposal distribution\n",
    "D = 2 #dimension of the parameter space\n",
    "samples, ar = run_chain(Nsteps,np.transpose(np.array([[-10]*D,[10]*D])),sigmaprop,D,twoDGauss,cov,invcov) #run the chain\n",
    "print 'acceptance ratio:', ar\n",
    "\n",
    "#Plot the chain on top of the 2D-density:\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.plot(samples[:,0],samples[:,1],'-',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the x-values of the samples:\n",
    "plt.plot(samples[:,0])\n",
    "plt.xlabel(r'sample number')\n",
    "plt.ylabel(r'$x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a 'Markov chain'. What do you think a markov chain should look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the plot to determine how many samples to throw out as \"burn-in\":\n",
    "burnin = 50\n",
    "non_burnin_samples = samples[burnin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the samples are not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the auto-correlation of the remaining samples:\n",
    "xmean=(np.mean(non_burnin_samples[:,0]))\n",
    "xvar=(np.var(non_burnin_samples[:,0]))\n",
    "ACL=np.array([(((non_burnin_samples[h:,0]-xmean)*(non_burnin_samples[:-h,0]-xmean)).sum())/(len(non_burnin_samples[:,0])-h)/xvar for h in range(1,50)])                              \n",
    "\n",
    "plt.plot(ACL)\n",
    "plt.xlabel(r'difference in sample number')\n",
    "plt.ylabel(r'correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the plot to determine the correlation length between samples\n",
    "corrlength = 20\n",
    "independentsamples = non_burnin_samples[::corrlength]\n",
    "print independentsamples.shape\n",
    "print 'effective acceptance ratio:', 1.*independentsamples.shape[0]/Nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot only the independent samples:\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.plot(independentsamples[:,0],independentsamples[:,1],'.',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have increased the acceptance ratio, but we have decreased the relative number of independent samples.\n",
    "\n",
    "A high acceptance ratio isn't always desirable. Can you think of any other reason why a high acceptance ratio might not be ideal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a multimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a multimodal distribution (remember that the normalization doesn't matter):\n",
    "def multimodal(samp,cov,invcov):\n",
    "    x = samp[0]\n",
    "    y = samp[1]\n",
    "    return twoDGauss(samp,cov,invcov) + 0.1*np.exp(-0.5*((x-5.)**2 + (y-5)**2))\n",
    "\n",
    "#Plot it:\n",
    "mm = multimodal([gridx,gridy],cov,invcov)\n",
    "plt.imshow(mm,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a chain:\n",
    "np.random.seed(15) # first try this\n",
    "#np.random.seed(11) # then try this.\n",
    "Nsteps = 200\n",
    "sigmaprop = 1.0\n",
    "D = 2\n",
    "samples, ar = run_chain(Nsteps,np.transpose(np.array([[-10]*D,[10]*D])),sigmaprop,D,multimodal,cov,invcov)\n",
    "print 'acceptance ratio:', ar\n",
    "\n",
    "#Plot the chain on top of the 2D-density:\n",
    "plt.imshow(mm,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.plot(samples[:,0],samples[:,1],'-',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the different seeds provided above. What do you notice? Does this change if you draw more random samples? Remember: an MCMC is guaranteeed to converge to the true distribution, but it is not guaranteed to do so fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginalizing over parameters is trivial; just ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_samples = samples[50::25] #remove burn-in phase and correlated samples\n",
    "\n",
    "x_values = independent_samples[:,0]\n",
    "\n",
    "plt.hist(x_values,range=(-10,10),bins=40,normed=True)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$P(x)$')\n",
    "plt.show()\n",
    "\n",
    "y_values = independent_samples[:,1]\n",
    "\n",
    "plt.hist(y_values,range=(-10,10),bins=40,normed=True)\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$P(y)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try again going to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard deviation in each direction for the D-dimensional Gaussian:\n",
    "sigma = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a chain:\n",
    "np.random.seed(15)\n",
    "D = 10\n",
    "Nsteps = 2000\n",
    "sigmaprop = 1.0\n",
    "samples, ar = run_chain(Nsteps,np.transpose(np.array([[-10]*D,[10]*D])),sigmaprop,D,DdimGauss,sigma,D)\n",
    "print 'acceptance ratio:', ar\n",
    "\n",
    "print 'dimension of the samples:', samples[0].shape\n",
    "\n",
    "#Plot the first dimension of the samples:\n",
    "plt.plot(samples[:,0])\n",
    "plt.xlabel(r'sample number')\n",
    "plt.ylabel(r'$x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think a markov chain should look like? \n",
    "\n",
    "Increase the number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we measure a sin wave with an amplitude and a frequency, so that we have data\n",
    "\n",
    "$d = x\\ cos(y t) + n$,\n",
    "\n",
    "where $n$ is some observational error. We will assume that we have several measurements and that the errors are Gaussian and independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate a set of Nobs observations:\n",
    "Nobs = 10\n",
    "sigma = 0.5 #the standard deviation of the Gaussian noise\n",
    "x = 2. #fix the true values of the two parameters\n",
    "y = 4.*2.*np.pi/Nobs\n",
    "print y\n",
    "#Draw noise realizations:\n",
    "np.random.seed(10)\n",
    "n = np.random.normal(0.,sigma,Nobs)\n",
    "\n",
    "#Observation times\n",
    "t = np.arange(Nobs)\n",
    "\n",
    "#Generate a data set:\n",
    "d = x*np.cos(y*t) + n\n",
    "\n",
    "plt.plot(d,'x')\n",
    "plt.plot(d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the likelihood (which is proportional to the posterior, since we are assuming flat priors):\n",
    "def likelihood(samp,d,t):\n",
    "    x = samp[0]\n",
    "    y = samp[1]\n",
    "    \n",
    "    res = (d-x*np.cos(y*t))**2 \n",
    "    \n",
    "    return np.exp(-0.5*(res.sum()/sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "#np.random.seed(None)\n",
    "Nsteps = 20000\n",
    "sigmaprop = 0.2\n",
    "D=2\n",
    "samples, ar = run_chain(Nsteps,np.array([[0,5],[0,np.pi]]),sigmaprop,D,likelihood,d,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'acceptance ratio:', ar\n",
    "plt.plot(samples[:,0],samples[:,1],'x',color='red')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples[:,0])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(samples[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unset the random seed above and run more chains. What do you notice? Do they converge to the same area? How fast? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = samples[:,0]\n",
    "\n",
    "plt.hist(x_values,range=(0,5),bins=20,normed=True)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$P(x)$')\n",
    "plt.show()\n",
    "\n",
    "y_values = samples[:,1]\n",
    "\n",
    "plt.hist(y_values,range=(0,np.pi),bins=40,normed=True)\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$P(y)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice: Run several chains with different starting values and make sure that they all converge to the same area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gibbs sampling: Instead of drawing samples from $P(x,y)$, draw alternately from $P(x|y)$ and $P(y|x)$.\n",
    "* Hamiltonian sampling: Think of the parameters as positions. Add a second set of unknown parameters, corresponding to momenta. Then follow the Hamiltonian equations of motion for some time to propose a new sample.\n",
    "* Various python modules exist that have some of these methods pre-implemented, e.g.:\n",
    "\n",
    "    - emcee (http://dan.iel.fm/emcee/current/)\n",
    "    - pymc (https://pymc-devs.github.io/pymc/)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
