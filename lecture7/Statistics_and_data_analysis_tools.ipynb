{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib \n",
    "% matplotlib inline\n",
    "% config InlineBackend.figure_format='retina'\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "matplotlib.rcParams['xtick.labelsize'] = 14.0\n",
    "matplotlib.rcParams['ytick.labelsize'] = 14.0\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "#from matplotlib.ticker import LogFormatterMathtext\n",
    "\n",
    "\n",
    "from scipy.stats import ks_2samp, entropy\n",
    "from scipy.integrate import simps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical properties of data\n",
    "\n",
    "Given a dataset it is useful to compute its statistical properties, as most physical measurements only are describable (and/or interesting) as part of a larger ensamble of data in an analysis.\n",
    "The underlying phsyics will determine what the expected form of the statistical properties are, but given a large number of datapoints most will tend towards a Gaussian distribution(see [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).\n",
    "\n",
    "In a bivariate case\n",
    "\n",
    "$$p(x,y) \\propto \\exp{ \\left( -\\frac{1}{2} (\\mathbf{x}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\mathbf{\\mu}) \\right) }$$\n",
    "\n",
    "where $\\mathbf{x} \\equiv \\begin{pmatrix}x\\\\y\\end{pmatrix}$ are the positions of the datapoints, their means are described by $\\mathbf{\\mu} \\equiv \\begin{pmatrix}\\mu_x\\\\\\mu_y\\end{pmatrix}$ and their covariance matrix is $\\Sigma \\equiv \\begin{pmatrix}\\sigma_x^2 & \\rho\\sigma_x\\sigma_y\\\\\\rho\\sigma_x\\sigma_y & \\sigma_y^2\\end{pmatrix}$. The [correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the two dimensions are defined as $\\rho \\equiv \\frac{\\sigma_x\\sigma_y}{\\sqrt{\\sigma_x^2\\sigma_y^2}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(dataset):\n",
    "    \n",
    "    print 'Mean of x-values ', \"%.2f\" % dataset[0].mean()\n",
    "    print 'Mean of y-values ', \"%.2f\" % dataset[1].mean()\n",
    "    print 'Standard deviation of x-values ', \"%.2f\" % dataset[0].std()\n",
    "    print 'Standard deviation of y-values ', \"%.2f\" % dataset[1].std()\n",
    "    print 'Correlation between x and y ', \"%.2f\" % np.corrcoef(dataset[0], dataset[1])[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = np.genfromtxt('dataset_0.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(d0[0], d0[1], lw=0,c='b')\n",
    "plt.xlim(15,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xticks([20,40, 60, 80, 100])\n",
    "plt.yticks([0, 20,40, 60, 80, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print its statistical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some more data and print their statistical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.genfromtxt('dataset_1.dat')\n",
    "d2 = np.genfromtxt('dataset_2.dat')\n",
    "\n",
    "print_stats(d1)\n",
    "print '\\n'\n",
    "print_stats(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have three datasets that appears to have pretty much the same properties.\n",
    "\n",
    "In situations like this it's useful to test how similar the datasets really are, or more formally what the probability is that the datapoints in the two sets are drawn from the same statistical distribution.\n",
    "This can be done using the [KS-test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_KS_test_prob(data1, data2):\n",
    "    \n",
    "    print ks_2samp(data1, data2)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy only has a 1-dimensional KS-test built in, so we'll have to run it separately for the x- and y-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_KS_test_prob(d0[0], d1[0])\n",
    "print_KS_test_prob(d0[0], d2[0])\n",
    "print_KS_test_prob(d1[0], d2[0])\n",
    "print_KS_test_prob(d0[1], d1[1])\n",
    "print_KS_test_prob(d0[1], d2[1])\n",
    "print_KS_test_prob(d1[1], d2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that if these points were drawn from the same underlying distributions all these p-values would have been $\\sim1$\n",
    "\n",
    "A simple check to see how similar these 2D-distributions are is to plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4.5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(d0[0], d0[1], lw=0, c='b')\n",
    "plt.xlim(15,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xticks([20,40, 60, 80, 100])\n",
    "plt.yticks([0, 20,40, 60, 80, 100])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(d1[0], d1[1], lw=0, c='g')\n",
    "plt.xlim(15,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xticks([20,40, 60, 80, 100])\n",
    "plt.yticks([0, 20,40, 60, 80, 100])\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(d2[0], d2[1], lw=0, c='r')\n",
    "plt.xlim(15,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xticks([20,40, 60, 80, 100])\n",
    "plt.yticks([0, 20,40, 60, 80, 100])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conlcusion, calculating statistical properties are only really useful as long as you're aware what those properties can (and can't) tell you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating parameters from noisy data\n",
    "\n",
    "Analysing data requires as much awareness about the signal you're trying to find as well as a grasp of whatever can contaminate your data, i.e. noise.\n",
    "\n",
    "Here a statistical approach, where you can include as much information as you have access to, tends to be the most successful.\n",
    "\n",
    "\n",
    "## Time for some whiteboard talk!\n",
    "\n",
    "Bayes' Theorem\n",
    "\n",
    "\n",
    "$$ p(A|B) = \\frac{p(A) p(B|A)}{p(B)} $$\n",
    "\n",
    "or in \"special\" notation\n",
    "\n",
    "$$ p(\\theta|d) = \\frac{\\pi(\\theta) L(d|\\theta)}{Z(d)} $$\n",
    "\n",
    "where $p(\\theta|d)$ is a posterior probability function which describes the probability of parameters $\\theta$ given some data $d$. This posterior depends on a likelihood $L(d|\\theta)$ (how likely are the data to be described by a parameter $\\theta$), a prior probability $\\pi(\\theta)$ (are there any constraints on the $\\theta$ already) and normalised by the evidence $Z(d)$ (how good is the assumed model at describing the data $d$).\n",
    "\n",
    "Assume a signal consisting of a one-dimensional Gaussian, so in this case $\\theta = \\mu$ only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_oneDim(x,m,s, a=1.):\n",
    "    G = np.exp(-np.square(x-m)/(2*np.square(s)))\n",
    "    normalisation = simps(G,x)\n",
    "    return a*G/normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.linspace(0,10,1000)\n",
    "\n",
    "mu_true = np.pi\n",
    "sigma_true = 0.3\n",
    "amplitude_true = 1.\n",
    "\n",
    "\n",
    "signal = Gaussian_oneDim(x_val, mu_true, sigma_true, amplitude_true)\n",
    "\n",
    "plt.plot(x_val, signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This signal will be hidden in noise, where the random process generating the noise can be described by a zero-mean Gaussian process with a known standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sigma = 1.4\n",
    "\n",
    "noise = np.random.normal(0, noise_sigma, x_val.size)\n",
    "\n",
    "data = noise + signal\n",
    "\n",
    "plt.plot(x_val, data)\n",
    "plt.plot(x_val, signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the property of this data that is known the best (at the moment) is the standard deviation of the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data.std(), noise_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore useful to define a likelihood function which tests for how Gaussian the noise is (remember $noise = data - signal$)\n",
    "\n",
    "\n",
    "It's helpful to work in the logarithm of probabilities (they tend to be easier to keep track of numerically)\n",
    "\n",
    "$$ \\log L(d|\\mu) = -\\frac{1}{2} \\int^{x_{max}}_{x_{min}}\\frac{(d(x)-Signal(x,\\mu))^2}{\\sigma_{noise}^2}dx$$\n",
    "\n",
    "(compare to [$\\chi^2$-test](https://en.wikipedia.org/wiki/Chi-squared_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data, x_data, mu_test, sigma_test=1., amplitude_test=1., noise_sigma_test=1.):\n",
    "    \n",
    "    logL = -0.5*simps(np.square((data - Gaussian_oneDim(x_data,mu_test,sigma_test,amplitude_test))/noise_sigma),x_data )\n",
    "\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logL = np.zeros_like(x_val)\n",
    "                          \n",
    "for i in xrange(x_val.shape[0]):\n",
    "\n",
    "    logL[i] = log_likelihood(data, x_val, x_val[i], sigma_test=sigma_true, \\\n",
    "                            amplitude_test=amplitude_true, noise_sigma_test=noise_sigma)\n",
    "\n",
    "plt.plot(x_val, logL, c='b')\n",
    "plt.axvline(np.pi, c='r', lw=2)\n",
    "plt.xlabel(r'$\\mu$', fontsize=16)\n",
    "plt.ylabel('logL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the likelihood of the data containing a signal with a mean $\\mu$, but it still doesn't take any prior information into account.\n",
    "\n",
    "For now, assume that the true $\\mu$ lies somewhere inside the range $a < \\mu < b$ where each point has equal probability. This gives\n",
    "\n",
    "$$ \\pi(\\mu) = \\frac{1}{b - a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior_uniform(input_points, x_min, x_max):\n",
    "    logprior = np.zeros_like(input_points)\n",
    "    \n",
    "    inside_range = np.logical_and(np.greater_equal(input_points, x_min), np.less_equal(input_points, x_max))\n",
    "    \n",
    "    logprior[inside_range] = -np.log(x_max - x_min)\n",
    "    \n",
    "    logprior[np.logical_not(inside_range)] = -np.inf\n",
    "    \n",
    "    return logprior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only thing missing from the complete posterior probability distribution is the evidence. Since the posterior is an \"actual\" probability distribution it needs to integrate to 1.\n",
    "\n",
    "$$ \\int^{\\infty}_{-\\infty} p(\\mu|d) d\\mu = 1 $$\n",
    "\n",
    "which therefore, from rearranging Bayes' Theorem\n",
    "\n",
    "$$Z(d) p(\\mu|d) = \\pi(\\mu) L(d|\\mu)$$ \n",
    "\n",
    "and since $Z(d)$ is independent of $\\mu$\n",
    "\n",
    "$$Z(d) \\int^{\\infty}_{-\\infty} p(\\mu|d) d\\mu = \\int^{\\infty}_{-\\infty}\\pi(\\mu) L(d|\\mu) d\\mu$$\n",
    "$$Z(d) = \\int^{\\infty}_{-\\infty}\\pi(\\mu) L(d|\\mu) d\\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior_1 = log_prior_uniform(x_val, 1., 6.)\n",
    "\n",
    "log_evidence_1 = np.log(simps(np.exp(log_prior_1 + logL), x_val))\n",
    "\n",
    "log_posterior_1 = log_prior_1 + logL - log_evidence_1\n",
    "\n",
    "plt.plot(x_val, log_posterior_1, c='r', label='posterior')\n",
    "plt.plot(x_val, log_prior_1, c='g', label='prior')\n",
    "plt.axvline(np.pi, c='k', lw=2, label=r'$\\mu_{true}$')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(r'$\\mu$', fontsize=16)\n",
    "plt.ylabel('log probability')\n",
    "plt.ylim(-3,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use other choises for the signal model (which comes in either through the likelihood or the prior), and then compare their ability to describe the data using the ratio of their evidences. This is called a [Bayes factor](https://en.wikipedia.org/wiki/Bayes_factor).\n",
    "\n",
    "As an example, we'll use a Gaussian prior on the signal parameter $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior_2 = np.log(Gaussian_oneDim(x_val, 3, 1.))\n",
    "\n",
    "log_evidence_2 = np.log(simps(np.exp(log_prior_2 + logL), x_val))\n",
    "\n",
    "log_posterior_2 = log_prior_2 + logL - log_evidence_2\n",
    "\n",
    "plt.plot(x_val, log_posterior_1, c='r', label='posterior 1')\n",
    "plt.plot(x_val, log_prior_1, c='g', label='uniform prior')\n",
    "plt.plot(x_val, log_posterior_2, c='m', label='posterior 2')\n",
    "plt.plot(x_val, log_prior_2, c='c', label='Gaussian prior')\n",
    "plt.axvline(np.pi, c='k', lw=2, label=r'$\\mu_{true}$')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(r'$\\mu$', fontsize=16)\n",
    "plt.ylabel('log probability')\n",
    "plt.ylim(-3,1)\n",
    "plt.show()\n",
    "\n",
    "print 'The uniform prior is preferred by a factor' , \"%.4f\" %np.exp(log_evidence_1-log_evidence_2)\n",
    "print 'The Gaussian prior is preferred by a factor' , \"%.4f\" %np.exp(log_evidence_2-log_evidence_1)\n",
    "\n",
    "print 'With a uniform prior, the posterior peaks at', \"%.4f\" %x_val[np.argmax(log_posterior_1)]\n",
    "print 'With a Gaussian prior, the posterior peaks at',  \"%.4f\" %x_val[np.argmax(log_posterior_2)]\n",
    "print 'The truth is ', \"%.4f\" %np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to measure how much information that has been gained from the data, in other words what you have learnt in going from the prior to the posterior probabilities.\n",
    "This is done through the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) defined as\n",
    "\n",
    "$$ D_{KL}(p||q) = \\int^{\\infty}_{-\\infty} p(\\theta|d) \\log \\left(\\frac{p(\\theta|d)}{\\pi(\\theta)}\\right) d\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_KL_divergence(log_prior, log_posterior, base_to_use=2.):\n",
    "    return entropy(np.exp(log_posterior), np.exp(log_prior), base=base_to_use)\n",
    "\n",
    "print 'With a uniform prior, the information gained is ', \"%.4f\" %print_KL_divergence(log_prior_1, log_posterior_1), ' bits'\n",
    "print 'With a Gaussian prior, the information gained is ',  \"%.4f\" %print_KL_divergence(log_prior_2, log_posterior_2), ' bits'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend this to 2D 😃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_to_test = np.logspace(-2,1,300)\n",
    "\n",
    "logL2 = np.zeros([x_val.shape[0], sigma_to_test.shape[0]])\n",
    "\n",
    "for i in xrange(x_val.shape[0]):\n",
    "    for j in xrange(sigma_to_test.shape[0]):\n",
    "\n",
    "        logL2[i,j] = log_likelihood(data, x_val, x_val[i], sigma_test=sigma_to_test[j],\\\n",
    "                                  amplitude_test=amplitude_true, noise_sigma_test=noise_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scale parameters like $\\sigma$, a useful prior is [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior) which is defined as $\\pi(\\sigma)\\propto \\frac{1}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_Jeffreys_prior(sigma_range):\n",
    "    \n",
    "    raw_prior = 1./sigma_range\n",
    "    normalisation = simps(raw_prior, sigma_range)\n",
    "    \n",
    "    return np.log(raw_prior/normalisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to do model comparison for the 2D case as well, so combine the priors used for $\\sigma$ with the priors used for $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_logPrior_1 = log_Jeffreys_prior(sigma_to_test)\n",
    "sigma_logPrior_2 = log_prior_uniform(sigma_to_test, x_val[1]-x_val[0], 1.)\n",
    "\n",
    "                                    \n",
    "twoD_logprior_1 = np.log(np.outer(np.exp(log_prior_1), np.exp(sigma_logPrior_1)))\n",
    "twoD_logprior_2 = np.log(np.outer(np.exp(log_prior_1), np.exp(sigma_logPrior_2)))\n",
    "twoD_logprior_3 = np.log(np.outer(np.exp(log_prior_2), np.exp(sigma_logPrior_1)))\n",
    "twoD_logprior_4 = np.log(np.outer(np.exp(log_prior_2), np.exp(sigma_logPrior_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the evidence for each model choice, now a 2D integral\n",
    "\n",
    "This allows the construction of the 2D posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoD_logevidence_1 =  np.log(simps(simps(np.exp(logL2+twoD_logprior_1), sigma_to_test), x_val))\n",
    "twoD_logevidence_2 =  np.log(simps(simps(np.exp(logL2+twoD_logprior_2), sigma_to_test), x_val))\n",
    "twoD_logevidence_3 =  np.log(simps(simps(np.exp(logL2+twoD_logprior_3), sigma_to_test), x_val))\n",
    "twoD_logevidence_4 =  np.log(simps(simps(np.exp(logL2+twoD_logprior_4), sigma_to_test), x_val))\n",
    "\n",
    "twoD_logPosterior_1 = logL2+twoD_logprior_1-twoD_logevidence_1\n",
    "twoD_logPosterior_2 = logL2+twoD_logprior_2-twoD_logevidence_2\n",
    "twoD_logPosterior_3 = logL2+twoD_logprior_3-twoD_logevidence_3\n",
    "twoD_logPosterior_4 = logL2+twoD_logprior_4-twoD_logevidence_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=np.meshgrid(sigma_to_test, x_val)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y,np.exp(twoD_logPosterior_1), cmap='cubehelix', lw=0.)\n",
    "ax.set_xlim(1e-2,1.2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y,np.exp(twoD_logPosterior_2), cmap='cubehelix', lw=0.)\n",
    "ax.set_xlim(1e-2,1.2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y,np.exp(twoD_logPosterior_3), cmap='cubehelix', lw=0.)\n",
    "ax.set_xlim(1e-2,1.2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X,Y,np.exp(twoD_logPosterior_4), cmap='cubehelix', lw=0.)\n",
    "ax.set_xlim(1e-2,1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Model 1 has evidence ', \"%.4f\" %np.exp(twoD_logevidence_1)\n",
    "print 'Model 2 has evidence ', \"%.4f\" %np.exp(twoD_logevidence_2)\n",
    "print 'Model 3 has evidence ', \"%.4f\" %np.exp(twoD_logevidence_3)\n",
    "print 'Model 4 has evidence ', \"%.4f\" %np.exp(twoD_logevidence_4)\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "print 'Model 1 is preferred over model 2 by', \"%.4f\" %np.exp(twoD_logevidence_1-twoD_logevidence_2)\n",
    "print 'Model 1 is preferred over model 3 by', \"%.4f\" %np.exp(twoD_logevidence_1-twoD_logevidence_3)\n",
    "print 'Model 1 is preferred over model 4 by', \"%.4f\" %np.exp(twoD_logevidence_1-twoD_logevidence_4)\n",
    "print 'Model 2 is preferred over model 1 by', \"%.4f\" %np.exp(twoD_logevidence_2-twoD_logevidence_1)\n",
    "print 'Model 2 is preferred over model 3 by', \"%.4f\" %np.exp(twoD_logevidence_2-twoD_logevidence_3)\n",
    "print 'Model 2 is preferred over model 4 by', \"%.4f\" %np.exp(twoD_logevidence_2-twoD_logevidence_4)\n",
    "print 'Model 3 is preferred over model 1 by', \"%.4f\" %np.exp(twoD_logevidence_3-twoD_logevidence_1)\n",
    "print 'Model 3 is preferred over model 2 by', \"%.4f\" %np.exp(twoD_logevidence_3-twoD_logevidence_2)\n",
    "print 'Model 3 is preferred over model 4 by', \"%.4f\" %np.exp(twoD_logevidence_3-twoD_logevidence_4)\n",
    "print 'Model 4 is preferred over model 1 by', \"%.4f\" %np.exp(twoD_logevidence_4-twoD_logevidence_1)\n",
    "print 'Model 4 is preferred over model 2 by', \"%.4f\" %np.exp(twoD_logevidence_4-twoD_logevidence_2)\n",
    "print 'Model 4 is preferred over model 3 by', \"%.4f\" %np.exp(twoD_logevidence_4-twoD_logevidence_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index1 = np.unravel_index(np.argmax(twoD_logPosterior_1), twoD_logPosterior_1.shape)\n",
    "max_index2 = np.unravel_index(np.argmax(twoD_logPosterior_2), twoD_logPosterior_2.shape)\n",
    "max_index3 = np.unravel_index(np.argmax(twoD_logPosterior_3), twoD_logPosterior_3.shape)\n",
    "max_index4 = np.unravel_index(np.argmax(twoD_logPosterior_4), twoD_logPosterior_3.shape)\n",
    "\n",
    "print 'Model 1 peaks at mean ', \"%.4f\" %x_val[max_index1[0]], ' and sigma ', \"%.4f\" %sigma_to_test[max_index1[1]]\n",
    "print 'Model 2 peaks at mean ', \"%.4f\" %x_val[max_index2[0]], ' and sigma ', \"%.4f\" %sigma_to_test[max_index2[1]]\n",
    "print 'Model 3 peaks at mean ', \"%.4f\" %x_val[max_index3[0]], ' and sigma ', \"%.4f\" %sigma_to_test[max_index3[1]]\n",
    "print 'Model 4 peaks at mean ', \"%.4f\" %x_val[max_index4[0]], ' and sigma ', \"%.4f\" %sigma_to_test[max_index4[1]]\n",
    "print 'The truth has mean ', \"%.4f\" %mu_true,  ' and sigma ', \"%.4f\" %sigma_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, this is easily extendable to signal models with more parameters (in this case also test for the best matching amplitude) but the current grid model isn't very computationally efficient with increased dimensionality. Tomorrow you'll learn about methods which has a much slower dimensional scaling, and which are therefore more suitable for high-dimensional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    " * 1 Repeat the 1D noisy-data test with a fixed $\\mu$, but varying $\\sigma$ \n",
    " \n",
    " * 2 The current measures of $\\mu$ and $\\sigma$ appear to vary quite a lot based on the choice of priors, find a threshold signal to noise level (either by varying the signal amplitude or $\\sigma_{noise}$) where this is no longer the case\n",
    " \n",
    " * 3 Are there other choices of models, either in the likelihood function or the prior distribution, which can give stronger inference on $\\mu$ and/or $\\sigma$ for a given (not too high) signal to noise ratio?\n",
    "\n",
    " * 4a Find other statistical tests which can show the differences and similarities between the datasaurus datasets\n",
    " * 4b Generalise the tests to two dimensions\n",
    " \n",
    " * 5 Compute the KL-divergence for the 2D noisy Gaussian models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
