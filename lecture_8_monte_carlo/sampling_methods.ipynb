{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576356df-6b35-471b-85e4-ffbbc4a73ed8",
   "metadata": {},
   "source": [
    "# Random sampling and Monte Carlo methods\n",
    "\n",
    "Authors: Maya Fishbach, adapted from Carl Haster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121d8b8-b504-4a2b-9fd3-f59ff932d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 3)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ad4f7-7644-4e72-9da8-be0691646974",
   "metadata": {},
   "source": [
    "## Why samples?\n",
    "\n",
    "* maximum likelihood is not enough for many applications, especially in high dimensions\n",
    "  * the probability contained in a given region of parameter space is the probability density $\\times$ volume\n",
    "  * the volume can be very large away from the maximum likelihood or maximum a posteriori\n",
    "* samples allow us to compute expectation values (\"Monte Carlo integrals\")\n",
    "  * moments of the distribution (mean, variance, etc.)\n",
    "* map out the probability distribution, find correlations, etc.\n",
    "  * sampling from the distribution can be much cheaper than evaluating the likelihood on a high-dimensional grid!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209f113-3b6c-4e33-8f55-9e5d155a2fbf",
   "metadata": {},
   "source": [
    "## Drawing samples from a PDF\n",
    "\n",
    "* pseudo-random number generators\n",
    "* Given a (pseudo)-random number from a known distribution, transform to a different distribution via:\n",
    "  * inverse transform sampling\n",
    "  * rejection sampling\n",
    "\n",
    "### Psuedo-random number generators\n",
    "\n",
    "Computers are deterministic. A pseudo-random number generator uses some formula whose outcome depends sensitively on the input, then iterate.\n",
    "\n",
    "Note: Need starting point for the iteration (random number seed), but that makes the result reproducible (good for testing and debugging).\n",
    "\n",
    "A popular pseudo-random number generator is a [linear congruential generator](https://en.m.wikipedia.org/wiki/Linear_congruential_generator). Such a generator is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a63604-3d55-4106-b130-685bd6e4e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rng(m=2**32, a=1103515245, b=12345):\n",
    "    \"\"\"Generate a sequence of (pseudo-)random numbers from a uniform distribution.\n",
    "    This function updates a given (pseudo-)random number to a new one.\n",
    "    \"\"\"\n",
    "    my_rng.current = (a*my_rng.current + b) % m\n",
    "    return my_rng.current/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b0c5e-57aa-4cc8-9a73-f43916a93997",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rng.current = 239485821 #setting the random number seed\n",
    "random_numbers = np.array([my_rng() for i in range(1000)]) #Draw a 1000 random numbers\n",
    "plt.hist(random_numbers,range=(0.,1.),bins=10) #plot a histogram\n",
    "plt.xlabel(\"random number\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55512f3f-dfe3-43e9-9cb0-eb5467503a23",
   "metadata": {},
   "source": [
    "In numpy, this is implemented in the [numpy.random](https://numpy.org/doc/stable/reference/random/index.html) module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75b2ca-4c16-40a8-af83-d8d9e6d9a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 123)\n",
    "# Generate 1000 random floats uniformly distributed over the range [0, 1)\n",
    "random_uniform = rng.random(1000) \n",
    "plt.hist(random_uniform,range=(0.,1.),bins=10) #plot a histogram\n",
    "plt.xlabel(\"random number\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb269de-f415-4245-8607-3a9a5431ffc0",
   "metadata": {},
   "source": [
    "### Inverse transform sampling\n",
    "\n",
    "If we can draw random numbers from a uniform distribution, we can draw from any distribution as long as we can calculate its CDF. \n",
    "\n",
    "* Calculate cumulative probability distribution (CDF): $\\mathrm{CDF}(x) = \\int_{-\\infty}^x \\mathrm{d}x' P(x)$\n",
    "* invert that function to get the inverse cumulative probability distribution (iCDF) such that if y = CDF(x), iCDF(y) = x\n",
    "* draw random values $y$ between 0 and 1\n",
    "* evaluate $\\mathrm{iCDF}(y)$ to find values of $x$ following the distribution $P(x)$\n",
    "\n",
    "For example, we can take our uniformly distributed random numbers and transform them to a set of random draws from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72f646-45b3-461f-9b89-e2285c74390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mu = 0, sigma = 1):\n",
    "\n",
    "    return np.exp(-(x-mu)**2/(2*sigma**2)) * (2*np.pi*sigma**2)**-0.5\n",
    "\n",
    "#the Gaussian cdf is known analytically, but for other distributions we may have to numerically integrate\n",
    "\n",
    "xs = np.linspace(-5,5,1000) #define array; +/- 5 sigma should capture almost 100% of the probability\n",
    "pdf_xs = Gaussian_pdf(xs) #evaluate Gaussian pdf on array \n",
    "cdf_xs = cumtrapz(pdf_xs, xs, initial = 0) #take the cumulative integral of the pdf to calculate the cdf\n",
    "inv_cdf = lambda y: np.interp(y, cdf_xs, xs) #calculate the inverse function of the cdf; for any number y between 0 and 1, return iCDF(y)\n",
    "\n",
    "#transform our 1000 draws from a uniform distribution (saved in random_uniform array)\n",
    "random_normal_its = inv_cdf(random_uniform)\n",
    "\n",
    "#plot \n",
    "plt.hist(random_normal_its,bins=20, density = True) #plot a histogram\n",
    "plt.plot(xs, pdf_xs)\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(0,0.5)\n",
    "\n",
    "plt.xlabel('random number')\n",
    "plt.ylabel('pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964de5-2d9f-4407-828c-a46b1299535f",
   "metadata": {},
   "source": [
    "Numpy already has a built in function to draw from a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc5d3a-f60d-4c59-be61-2c690dfbfae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of 1000 random floats drawn from a standard normal (Gaussian) distribution\n",
    "rng = np.random.default_rng(seed = 123)\n",
    "random_normal_np = rng.standard_normal(10000) \n",
    "\n",
    "plt.hist(random_normal_np,bins=20, density = True) #plot a histogram\n",
    "\n",
    "plt.plot(xs, pdf_xs) #plot analytic pdf\n",
    "\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel(\"random number\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700ae57-fd12-4e10-be2a-bc7704b3cd31",
   "metadata": {},
   "source": [
    "## Rejection sampling\n",
    "\n",
    "In more than one dimension, there is no well-defined cumulative distribution function. Thus, we have to come up with a new idea:\n",
    "\n",
    "* Sample (some region of) N-dimensional space following a simple PDF $Q(x)$ (we will use a uniform distribution).\n",
    "* Add an acceptance-rejection step that accepts the sample $x$ with probability $P(x)/(c\\ Q(x))$.\n",
    "* Choose $c$ such that $P(x) < c\\ Q(x)$ for all $x$ (but ideally not too large).\n",
    "\n",
    "We will use a two-dimensional Gaussian in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d362794-6446-41a8-9a98-d1a0c7741900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a two-dimensional grid for plotting purposes:\n",
    "xvals = np.arange(-10,10.01,0.01)\n",
    "yvals = np.arange(-10,10.01,0.01)\n",
    "gridx, gridy = np.meshgrid(xvals,yvals)\n",
    "\n",
    "#Define the two-dimensional covariance matrix:\n",
    "cov_xx = 6.\n",
    "cov_yy = 1.5\n",
    "cov_xy = 1.0\n",
    "cov = np.array([[cov_xx,cov_xy],[cov_xy,cov_yy]])\n",
    "invcov = np.linalg.inv(cov) #precompute the inverse covariance matrix (because it can get expensive)\n",
    "\n",
    "#define a two-dimensional Gaussian centered at 0 with a given covariance matrix\n",
    "def twoDGauss(samp,cov,invcov):\n",
    "    x = samp[0] \n",
    "    y = samp[1]\n",
    "    det = np.linalg.det(cov)\n",
    "    norm = 1./(2.*np.pi)/det**0.5\n",
    "    return norm*np.exp(-0.5*(x*(invcov[0,0]*x + invcov[0,1]*y) + y*(invcov[1,0]*x + invcov[1,1]*y)))\n",
    "\n",
    "#Evaluate the Gaussian at all grid points and plot it:\n",
    "G = twoDGauss([gridx,gridy],cov,invcov)\n",
    "\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = r'$p(x,y)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac8e8e-22de-4cb1-9067-6a01f67b4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define drawing step from proposal distribution:\n",
    "def draw_sample(rng = np.random.default_rng(seed = 123)):\n",
    "    return rng.uniform(low = -10, high = 10, size = 2) #two-dimensional uniform distribution\n",
    "\n",
    "#Define the acceptance-rejection step:\n",
    "c = 40 #as defined in equations above\n",
    "D = 2 #dimension of the parameter space\n",
    "\n",
    "def AcceptReject(c,D,P,*Pargs, rng = np.random.default_rng(seed = 123)):\n",
    "    \"\"\"This function returns `True' if the sample is accepted and `False' if not. We use a\n",
    "    variable-length argument list `*Pargs' to be able to use any probability function `P' that we\n",
    "    might come up with\"\"\"\n",
    "    Pval = P(*Pargs)\n",
    "    proposalval = 1/20**D #The proposal density is 1/20*1/20\n",
    "    prob = Pval/(c*proposalval) #acceptance probability\n",
    "    return rng.choice([True,False],p=[prob,1-prob]) #accept with probability prob, reject with probability 1-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9802d67-68bb-45eb-8d66-fb34f1105a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw some samples from the proposal distribution:\n",
    "N = 10000 #number of samples to draw\n",
    "count = 0 #count the accepted samples\n",
    "samples = [] #store the accepted samples\n",
    "\n",
    "for i in range(N):\n",
    "    samp = draw_sample() #draw a sample from the proposal distribution\n",
    "    acc = AcceptReject(c,D,twoDGauss,samp,cov,invcov)\n",
    "    if acc:\n",
    "        count += 1\n",
    "        samples.append(samp)\n",
    "samples = np.array(samples)\n",
    "\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = r'$p(x,y)$')\n",
    "plt.plot(samples[:,0],samples[:,1],'.',color='red', alpha = 0.3)\n",
    "plt.show()\n",
    "print('acceptance ratio:', count/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701d17e-6e9b-4439-9eef-6e07dffb2aa7",
   "metadata": {},
   "source": [
    "Let's calculate some moments of the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553127a8-0d71-4013-918e-a94f4fa85b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(samples)\n",
    "print('mean x:', np.mean(samples[:,0]))\n",
    "print('mean y:', np.mean(samples[:,1]))\n",
    "print('variance in x-direction:', np.mean(samples[:,0]**2))\n",
    "print('variance in y-direction:', np.mean(samples[:,1]**2))\n",
    "print('covariance of x and y:', np.mean(samples[:,0]*samples[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e077b5-0192-45ee-823d-12ee729c6a93",
   "metadata": {},
   "source": [
    "Compare to the covariance matrix we defined above. \n",
    "\n",
    "Now let's see what happens if we go to higher dimensions. For simplicity, we use a symmetric Gaussian without correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cc012-b880-4fe7-9fed-e1eaef0d0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the D-dimensional Gaussian:\n",
    "sigma = 2. #standard deviation in all dimensions\n",
    "def DdimGauss(xvec,sigma,D):\n",
    "    det = sigma**(2*D)\n",
    "    norm = 1./(2.*np.pi)**(D/2.)/det**0.5\n",
    "    return norm*np.exp(-0.5*np.dot(xvec,xvec/sigma**2))\n",
    "\n",
    "#Define the drawing step:\n",
    "def draw_sample_Ddim(D, rng = np.random.default_rng()):\n",
    "    return rng.uniform(low = -10, high = 10, size = D) #D-dimensional uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ffcf5-8340-484c-b56d-b1c047387ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw some samples:\n",
    "N = 10000 #number of samples to draw\n",
    "count = 0 #count the accepted samples\n",
    "samples = [] #store the accepted samples\n",
    "D = 3\n",
    "\n",
    "#Here we are cheating a bit: Since we know what the maximum of the D-dimensional PDF is, we can\n",
    "#calculate the optimal value of c to use:\n",
    "c = DdimGauss(np.zeros(D),sigma,D)*20.**D #the Gaussian reaches its max value at the mean (= 0). \n",
    "\n",
    "for i in range(N):\n",
    "    samp = draw_sample_Ddim(D)\n",
    "    acc = AcceptReject(c,D,DdimGauss,samp,sigma,D)\n",
    "    if acc:\n",
    "        count += 1\n",
    "        samples.append(samp)\n",
    "\n",
    "print('acceptance ratio:', count/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3f13a-d645-468f-9bda-ef14697cf61b",
   "metadata": {},
   "source": [
    "Rerun the cell for different choices of the dimension D. What happens to the acceptance ratio?\n",
    "\n",
    "This problem is known as \"the curse of dimensionality\". What do you think we could do to increase the acceptance ratio?\n",
    "\n",
    "Until now we have been drawing samples randomly in some range without using any 'past experience' of where samples have been accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97082f-c8dd-4db1-9c54-af8b4888e14e",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo: Metropolis Hastings\n",
    "[See Metropolis et al. (1953)](https://bayes.wustl.edu/Manual/EquationOfState.pdf)\n",
    "\n",
    "Markov Chain: Draw random samples in an ordered chain:\n",
    "\n",
    "* \"Markov\" property: Each sample depends on the sample before, but not on any other samples\n",
    "* Pro: Curse of dimensionality less severe\n",
    "* Con: Successive samples are correlated -> need to discard many of them to obtain a set of independent samples\n",
    "\n",
    "Specific example: Metropolis-Hastings for drawing from distribution $P$.\n",
    "\n",
    "* Select a \"proposal distribution\" $Q(x'|x)$ to draw a new sample $x'$ given a previous sample $x$. Below, we use a Gaussian distribution centered at $x$ for the proposal distribution. \n",
    "* If $P(x') \\geq P(x)$, accept the new sample (we are going \"uphill\"). \n",
    "* If $P(x') < P(x)$, accept the new sample with probability $(P(x')/P(x))(Q(x|x')/Q(x'|x))$. Note that if Q is Gaussian, this simplifies to $P(x')/P(x)$ because a Gaussian is symmetric.  \n",
    "* If a sample is rejected, the chain stays at the same position (i.e., the old sample is repeated).\n",
    "* Note that the normalization of $P$ doesn't matter. Very helpful when we don't know the normalization! \n",
    "\n",
    "Let's try again to sample from a two-dimensional Gaussian, this time with MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c82048-ddc1-44df-8834-d610e8eee583",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = r'$p(x,y)$')\n",
    "plt.title('target distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c7ddb-8d34-4d62-a157-5fde46e99d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the proposal step (we are using a symmetric Gaussian proposal density in D dimensions):\n",
    "def proposal(oldsamp, sigmaprop, D, rng = np.random.default_rng()):\n",
    "    '''Propose a new sample based on the current sample according to a symmetric Gaussian proposal pdf\n",
    "    \n",
    "    inputs:\n",
    "    -oldsamp - current sample, D-dimensional array\n",
    "    -sigmaprop - standard deviation (in all dimensions) of the Gaussian proposal \n",
    "    -D - number of dimensions\n",
    "    -rng - random number generator\n",
    "\n",
    "    returns:\n",
    "    -newsamp - proposed new sample, D-dimensional array\n",
    "    '''\n",
    "    newsamp = oldsamp + sigmaprop * rng.normal(size = D)\n",
    "    return newsamp\n",
    "\n",
    "#Define the acceptance-rejection step, return the new sample and a boolean that tells us whether\n",
    "#or not the new sample was accepted):\n",
    "def accept(rng, newsamp,oldsamp,paramrange,P,*Pargs):\n",
    "    '''acceptance-rejection step of MCMC\n",
    "    \n",
    "    inputs:\n",
    "    -rng -- random number generator\n",
    "    -newsamp -- proposed new sample, D-dimensional array\n",
    "    -oldsamp -- old sample, D-dimensional array\n",
    "    -paramrange -- acceptable range of samples; array with shape (D, 2)\n",
    "    -P -- target probability distribution to draw from (doesn't have to be normalized); function\n",
    "    -*Pargs -- variable length argument, parameters that define target P\n",
    "\n",
    "    returns:\n",
    "    -acc -- a boolean that tells us whether new sample was accepted\n",
    "    -next sample in the chain (either oldsamp or newsamp) -- D-dimensional array\n",
    "    '''\n",
    "    #first check if newsamp is in desired range, i.e.\n",
    "    #if every dim of newsamp > lower bound in paramrange\n",
    "    #and if every dim of new samp < upper bound in paramrange\n",
    "    if not ((np.array([p1 - p2 for p1, p2 in zip(newsamp, np.transpose(paramrange)[:][0])])>0).all() \\\n",
    "            and (np.array([p1 - p2 for p1, p2 in zip(np.transpose(paramrange)[:][1],newsamp)])>0).all()): \n",
    "        acc = False #if newsamp is not in desired range paramrange, reject (acc = False) and return old sample\n",
    "        return acc, oldsamp\n",
    "\n",
    "    #compute target distribution at proposed sample (newsamp) and and current sample (oldsamp)\n",
    "    newprob = P(newsamp,*Pargs) \n",
    "    oldprob = P(oldsamp,*Pargs) \n",
    "    \n",
    "    #if proposed sample is \"uphill\" -- accept \n",
    "    if newprob >= oldprob:\n",
    "        acc = True \n",
    "        return acc, newsamp\n",
    "\n",
    "    #if not, accept with probability newprob/oldprob\n",
    "    else:\n",
    "        prob = newprob/oldprob\n",
    "        acc = rng.choice([True,False],p=[prob,1-prob])\n",
    "        return acc, acc*newsamp + (1 - acc)*oldsamp #Note that this is either newsamp or oldsamp\n",
    "\n",
    "#Define function that runs an entire chain:\n",
    "def run_chain(rng, steps,paramrange,sigmaprop,D,P,*Pargs):\n",
    "    '''Runs an MCMC chain of samples from target distribution\n",
    "\n",
    "    inputs:\n",
    "    -rng -- random number generator\n",
    "    -steps -- number of steps to take in chain; int\n",
    "    -paramrange -- acceptable range of samples; array with shape (D, 2). In dimension i, paramrange[i][0] gives lower bound and paramrange[i][1] gives upper bound.\n",
    "    -sigmaprop -- standard deviation of symmetric Gaussian proposal distribution; float\n",
    "    -D -- number of dimensions; int\n",
    "    -P -- target probability distribution to draw from (doesn't have to be normalized); function\n",
    "    -*Pargs -- variable length argument, parameters that define target P\n",
    "\n",
    "    output:\n",
    "    -samples -- samples from target distribution; array of length steps\n",
    "    -ar --  acceptance ratio; float\n",
    "    '''\n",
    "    oldsamp=np.array([rng.uniform(paramrange[d][0],paramrange[d][1]) for d in range(D)])#Draw a random starting point in acceptable range\n",
    "    count = 0 #Count the number of accepted samples\n",
    "    samples = [oldsamp] #Store all samples\n",
    "    for i in range(steps):\n",
    "        newsamp = proposal(oldsamp,sigmaprop,D, rng) #Propose a new sample\n",
    "        acc, newsamp = accept(rng, newsamp,oldsamp,paramrange,P,*Pargs) #decide whether or not to accept it\n",
    "        samples.append(newsamp) #Add the sample to the list of samples\n",
    "        if acc:\n",
    "            count += 1\n",
    "        oldsamp = newsamp #Move to the new sample\n",
    "    ar = count/steps #compute the acceptance ratio\n",
    "    return np.array(samples), ar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef7d5e-7e26-4c9a-9443-4360da5efca8",
   "metadata": {},
   "source": [
    "Now let's run our chain. Recall we are trying to draw samples from a twoDGauss with covariance matrix defined above as cov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ff7a2-bc15-407c-a68e-50812ecbada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 13434)\n",
    "\n",
    "Nsteps = 5000 #number of steps to run the chain for\n",
    "paramrange = np.transpose(np.array([[-10]*D,[10]*D])) #acceptable range to sample in\n",
    "sigmaprop = 1 #width of the proposal distribution\n",
    "D = 2 #dimension of the parameter space\n",
    "Ptarget = twoDGauss #target distribution\n",
    "#Pargs here is cov, invcov\n",
    "samples, ar = run_chain(rng, Nsteps,paramrange,sigmaprop,D,Ptarget,cov,invcov) #run the chain\n",
    "print('acceptance ratio:', ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0eeb8-8465-4b34-b7c5-ece187fb29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the chain on top of the 2D-density:\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = 'target density')\n",
    "plt.plot(samples[:,0],samples[:,1],marker = '.', ls = '-',color='red', alpha = 0.1)\n",
    "plt.plot(samples[0,0], samples[0,1], marker = '*', color = 'black') #first sample\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde4cab-eaa5-4374-9ffc-780c02e0e283",
   "metadata": {},
   "source": [
    "Plot the x values and y values of the samples separately as a function of step number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433566c-4057-4d15-b546-83d50684dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(samples[:,0])\n",
    "plt.xlabel(r'sample number')\n",
    "plt.ylabel(r'$x$')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(samples[:,1])\n",
    "plt.xlabel(r'sample number')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c8d91-76d2-4ddb-a1bb-9f129b46a745",
   "metadata": {},
   "source": [
    "This is called a \"Markov chain.\"\n",
    "\n",
    "Note that:\n",
    "* it takes the chain some time to \"burn in\" and forget its initial value. We can throw away the first several samples.\n",
    "* Even after burn-in, the samples are not independent. We can keep only 1 in every N samples where N is the correlation length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b65b38-1efb-4939-abe6-c240a98cbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the plot to determine how many samples to throw out as \"burn-in\":\n",
    "burnin = 100\n",
    "non_burnin_samples = samples[burnin:]\n",
    "\n",
    "#calculate the auto-correlation of the remaining samples (in the x dimension):\n",
    "xmean=(np.mean(non_burnin_samples[:,0]))\n",
    "xvar=(np.var(non_burnin_samples[:,0]))\n",
    "\n",
    "Nsamps_non_burnin = len(non_burnin_samples[:,0])\n",
    "#for every h, calculate the normalized correlation between samples \"h\" apart.\n",
    "#This is Cov(X_i, X_{i+h})/xvar, or approximately\n",
    "#sum_{i=1}^{N-h}((samp[i+h]-mean)*(samp[i]-mean)) / (N-h)/ xvar\n",
    "#we don't want to consider h that is too high because then we have a very noisy estimate. \n",
    "autocorr = np.array([np.sum((non_burnin_samples[h:,0]-xmean)*(non_burnin_samples[:-h,0]-xmean))/(Nsamps_non_burnin-h)/xvar for h in range(1,100)])                              \n",
    "\n",
    "plt.plot(autocorr)\n",
    "plt.xlabel(r'difference in sample number')\n",
    "plt.ylabel(r'correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757f97e-9bfa-4974-a869-afe71e95b6b0",
   "metadata": {},
   "source": [
    "We can estimate the integrated autocorrelation time as 1 + 2*(the sum of autocorr over h). See, for example, the explanation in the [emcee tutorial](https://emcee.readthedocs.io/en/stable/tutorials/autocorr/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd27c3-8ec3-41ed-8c34-9f63bbda3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_time = 1+2*np.sum(autocorr)\n",
    "autocorr_time = round(autocorr_time)\n",
    "print(f'autocorrelation time is {autocorr_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e58ed1-79af-4df9-999a-03808231b2b5",
   "metadata": {},
   "source": [
    "We want to keep only the \"independent samples,\" throwing away those ones that are within an autocorrelation time of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e5749-f7bb-4fc9-8a5c-ecf527c6493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "independentsamples = non_burnin_samples[::autocorr_time]\n",
    "print(f\"we only have {independentsamples.shape[0]} independent samples\")\n",
    "print('effective acceptance ratio:', independentsamples.shape[0]/Nsteps)\n",
    "\n",
    "#Plot only the independent samples:\n",
    "plt.imshow(G,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = \"target density\")\n",
    "plt.plot(independentsamples[:,0],independentsamples[:,1],'.',color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb16d42-4ab8-49ea-8680-0d0bfffa4825",
   "metadata": {},
   "source": [
    "We have increased the acceptance ratio compared to the rejection sampling, but we have decreased the relative number of independent samples.\n",
    "\n",
    "A high acceptance ratio isn't always desirable. Can you think of any other reason why a high acceptance ratio might not be ideal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4e882-e2ed-460b-8019-a9dae0d18b60",
   "metadata": {},
   "source": [
    "### Let's try a multimodal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01cbc5-9dbe-4d81-a959-4c3e109e1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a multimodal distribution (remember that the normalization doesn't matter):\n",
    "def multimodal(samp,cov,invcov,mu2 = 5):\n",
    "    return twoDGauss(samp,cov,invcov) + 0.2*twoDGauss(np.array(samp)-mu2, cov, invcov)\n",
    "\n",
    "#Plot it:\n",
    "mm = multimodal([gridx,gridy],cov,invcov)\n",
    "plt.imshow(mm,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b215351-1d32-4edc-9190-eb50b0431464",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed = 13434) # first try this\n",
    "#rng = np.random.default_rng(seed = 9230) # then try this\n",
    "\n",
    "Nsteps = 10000 #number of steps to run the chain for\n",
    "\n",
    "Ptarget = multimodal #target distribution\n",
    "samples, ar = run_chain(rng, Nsteps,paramrange,sigmaprop,D,Ptarget,cov,invcov) #run the chain\n",
    "print('acceptance ratio:', ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61f307-08ad-4635-a407-78bda43eb4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the chain on top of the 2D-density:\n",
    "plt.imshow(mm,cmap=plt.cm.Blues,extent=[xvals.min(),xvals.max(),yvals.max(),yvals.min()])\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.colorbar(label = 'target density')\n",
    "plt.plot(samples[:,0],samples[:,1],marker = '.', ls = '-',color='red', alpha = 0.05)\n",
    "plt.plot(samples[0,0], samples[0,1], marker = '*', color = 'black') #first sample\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a820345-291f-4c65-a6c6-1548a789496f",
   "metadata": {},
   "source": [
    "Use the different seeds provided above. What do you notice? Does this change if you draw more random samples? Remember: an MCMC is guaranteeed to converge to the true distribution, but it is not guaranteed to do so fast.\n",
    "\n",
    "Marginalizing over parameters is trivial; just ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1159b2-ef06-4f1d-a0b1-37b06b845905",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_samples = samples[100::40] #remove burn-in phase and correlated samples\n",
    "\n",
    "x_values = independent_samples[:,0]\n",
    "\n",
    "plt.hist(x_values,range=(-10,10),bins=20,density=True)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$p(x)$')\n",
    "plt.show()\n",
    "\n",
    "y_values = independent_samples[:,1]\n",
    "\n",
    "plt.hist(y_values,range=(-10,10),bins=20,density=True)\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$p(y)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cabca4-c4f7-468e-bff4-98902e58f21e",
   "metadata": {},
   "source": [
    "A good way to visualize both the joint and marginal distributions is with a corner plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727e55c-bbe2-4121-a7fa-3cd724eb5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "corner.corner(independent_samples, labels = ['x', 'y']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf56db-d05b-475c-a1ad-562eadc389a6",
   "metadata": {},
   "source": [
    "Now let's try again going to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6d87-1bd9-4622-96a1-535449f8a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard deviation in each direction for the D-dimensional Gaussian:\n",
    "sigma = 2.\n",
    "\n",
    "#Run a chain:\n",
    "rng = np.random.default_rng(seed = 43895)\n",
    "D = 10 #10 dimensions\n",
    "Nsteps = 5000\n",
    "sigmaprop = 1\n",
    "samples, ar = run_chain(rng,Nsteps,np.transpose(np.array([[-10]*D,[10]*D])),sigmaprop,D,DdimGauss,sigma,D)\n",
    "print('acceptance ratio:', ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cf56d-db3a-4849-a03b-4e2f5b6e5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the first dimension of the samples:\n",
    "plt.plot(samples[:,0])\n",
    "plt.xlabel(r'sample number')\n",
    "plt.ylabel(r'$x$')\n",
    "plt.show()\n",
    "\n",
    "#make a corner plot (the most boring corner plot ever)\n",
    "corner.corner(samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87879a89-b556-4780-a33c-2a555b4860e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
