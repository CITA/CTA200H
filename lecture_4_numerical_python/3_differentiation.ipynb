{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3637973c",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "This section focuses on ways to numerically approximate the derivative of a function $f(x)$ with respect to an independent variable $x$. If we know $f$ explicitly we can use symbolic manipulations find $f'=\\frac{\\text{d}f}{\\text{d}x}$. Let's say that instead we only have data $f(x_n)$ sampled at $N$ points $x_0,x_1,...,x_n,...,x_{N-1}$. Taylor expanding $f$ at a point $x_n+h$ close to a given node $x_n$,\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x_n+h)=f(x_n) + hf'(x_n) + \\frac{1}{2}h^2f''(x_n) + \\mathcal{O}(h^3)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Taking $h$ to be the separation between successive grid points, ``finite difference'' methods use taylor expansions to approximate derivatives with an error that scales with some power $h$. For example, the first order forward finite difference approximation writes\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{h}[f(x_n+h)-f(x_n)] + \\mathcal{O}(h).\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, we might write\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{h}[f(x_n)-f(x_n-h)] + \\mathcal{O}(h).\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, both forward and backward finite differences perform poorly. A better approach is to combine the Taylor expansions for $f(x_n+h)$ and $f(x_n-h)$ to produce the \"centred'' finite difference approximation\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{2h}[f(x_n+h)-f(x_n-h)] +  \\mathcal{O}(h^2).\n",
    "\\end{equation}\n",
    "\n",
    "Of course we can't apply this formula to the endpoints $x_0$ and $x_{N-1}$ of the grid, since there we don't have $x_n-h$ and $x_n+h$ (respectively). The following function applies this centred differences approximation on interior points, and forward/backward approximations on the endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace516dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':16})\n",
    "\n",
    "def fundiff(ff,xx):\n",
    "    '''\n",
    "        computes the second-order finite differences approximation to the derivative of a \n",
    "        function f with values sampled at a grid-points xx, assuming that the grid-points\n",
    "        in xx are evenly spaced\n",
    "    '''\n",
    "    # compute array of grid-spacing h, and check that xx are all evenly spaced:\n",
    "    hh = xx[1] - xx[0]\n",
    "    if np.any(abs(np.diff(xx) - hh)>1e-14):\n",
    "        raise ValueError('expected evenly spaced grid')\n",
    "    \n",
    "    # initialize array to hold derivative values:\n",
    "    dfdx = np.zeros_like(ff)\n",
    "    \n",
    "    # use centred differences to compute derivatives at interior points:\n",
    "    dfdx[1:-1] = (ff[2:] - ff[:-2])/hh/2.\n",
    "    \n",
    "    # apply forward/backward formulae at endpoints:\n",
    "    dfdx[0] = (ff[1] - ff[0])/hh\n",
    "    dfdx[-1]= (ff[-1]- ff[-2])/hh\n",
    "    return dfdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e002514",
   "metadata": {},
   "source": [
    "Because this function applies a less accurate finite difference approximation at the boundaries, we expect the error to be larger there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0097037",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0,1,100)\n",
    "ff = np.cos(20*xx)*np.exp(-2*xx)\n",
    "dfdx = -20*np.sin(20*xx)*np.exp(-2*xx) - 2*np.cos(20*xx)*np.exp(-2*xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf7c7e",
   "metadata": {},
   "source": [
    "The numpy function np.gradient uses much the same approach as above, but with some more bells and whistles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(2,1,figsize=(12,9),sharex=True)\n",
    "ax[0].plot(xx,fundiff(ff,xx),label='fundiff')\n",
    "ax[0].plot(xx,np.gradient(ff,xx),label='np.gradient')\n",
    "ax[0].plot(xx,np.gradient(ff,xx,edge_order=2),label='np.gradient (2nd order edges)')\n",
    "ax[1].plot(xx,fundiff(ff,xx)-dfdx)\n",
    "ax[1].plot(xx,np.gradient(ff,xx)-dfdx)\n",
    "ax[1].plot(xx,np.gradient(ff,xx,edge_order=2)-dfdx)\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel(\"$f'(x)$\")\n",
    "ax[1].set_ylabel('Residual')\n",
    "ax[0].legend(ncol=3,fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0950e",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The second-order, forward, centred, and backward finite difference approximations for the \\emph{second} derivatives of a function are\n",
    "\\begin{align}\n",
    "    f''(x)\n",
    "    &\\simeq\\frac{1}{h^2}[f(x + 2h) -2 f(x+h) + f(x)] \\hspace{2em} \\text{(forward)},\n",
    "\\\\\n",
    "    &\\simeq\\frac{1}{h^2}[f(x + h) -2f(x) + f(x - h)] \\hspace{2em} \\text{(centred)},\n",
    "\\\\\n",
    "    &\\simeq\\frac{1}{h^2}[f(x) - 2 f(x -h) + f(x - 2h)] \\hspace{2em} \\text{(backward)}.\n",
    "\\end{align}\n",
    "Write a function that computes the second derivative of a function sampled at grid-points with a user-supplied constant separation h. Test it on the function considered above, and compare with the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86d5d5",
   "metadata": {},
   "source": [
    "# Spectral methods\n",
    "\n",
    "Finite difference methods can be extended to higher orders in accuracy, but they are far from the only approach to computing numerical derivatives. Spectral methods take the approach of expanding a function as\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\sum_j c_j \\phi_j(x),\n",
    "\\end{equation}\n",
    "\n",
    "where $c_j$ are constant coefficients, and $\\phi_j(x)$ are basis functions satisfying the same boundary conditions as $f$. If the coefficients $c_j$ are known, then $f'(x)$ can be computed from the basis functions as\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x) = \\sum_j c_j \\phi_j'(x).\n",
    "\\end{equation}\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose $f(x)$ is periodic on the domain of interest. Then the appropriate choice of $\\phi_j$ would be Fourier modes:\n",
    "\n",
    "\\begin{equation}\n",
    "    f=\\sum_{n=-\\infty}^\\infty c_n \\exp[\\text{i}n x]\n",
    "     =\\sum_{n=-\\infty}^\\infty c_n[\\cos(n x) + \\text{i}\\sin(nx)],\n",
    "\\end{equation}\n",
    "\n",
    "and so \n",
    "\n",
    "\\begin{equation}\n",
    "    f'=\\sum_{n=-\\infty}^\\infty \\text{i}nc_n \\exp[\\text{i}n x]\n",
    "     =\\sum_{n=-\\infty}^\\infty nc_n[\\text{i}\\cos(n x) - \\sin(nx)],\n",
    "\\end{equation}\n",
    "\n",
    "where $c_n$ are (potentially complex-valued) coefficients that can be determined using fast Fourier transforms:\n",
    "\n",
    "\\begin{equation}\n",
    "    c_n\\simeq\n",
    "    \\frac{1}{N}\\sum_{k=0}^{N-1}f(x_k)\\exp\\left[\\frac{-2\\pi\\text{i} nk}{N}\\right],\n",
    "\\end{equation}\n",
    "\n",
    "for $f$ sampled on the $N$ grid-points $x_0,...,x_{N-1}$. The following function differentiates a periodic function using Fourier transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf26029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft,fftshift\n",
    "\n",
    "def dfft(ff,xx):\n",
    "    N = xx.size\n",
    "    cn= fftshift(fft(ff)*2*np.pi/N) # fftshift \"unwraps\" a Nyquist shift introduced by fft\n",
    "    nn= np.arange(N) - int(N/2)\n",
    "    \n",
    "    # create 2D meshes of grid-points and indices/coefficients (not strictly \n",
    "    # necessary, but it makes the following reconstruction easier):\n",
    "    X,Nn = np.meshgrid(xx,nn,indexing='ij')\n",
    "    Cn = np.meshgrid(xx,cn,indexing='ij')[1]\n",
    "        \n",
    "    # compute derivative:\n",
    "    dfdx = np.sum(1.j*Nn*Cn*np.exp(2.j*np.pi*X*Nn),axis=1)\n",
    "    return dfdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87f6e9",
   "metadata": {},
   "source": [
    "This approach works much better than finite differences, but only when the sampled function is periodic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0,1,1000,endpoint=False)\n",
    "\n",
    "ff = np.cos(5*2*np.pi*xx)*np.sin(3*2*np.pi*xx)\n",
    "dfdx= -10*np.pi*np.sin(10*np.pi*xx)*np.sin(6*np.pi*xx) \\\n",
    "      +6* np.pi*np.cos(10*np.pi*xx)*np.cos(6*np.pi*xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(2,1,figsize=(12,9),sharex=True)\n",
    "ax[0].plot(xx,dfft(ff,xx),label='fft')\n",
    "ax[0].plot(xx,np.gradient(ff,xx),label='np.gradient')\n",
    "ax[0].plot(xx,dfdx,label='truth')\n",
    "ax[1].plot(xx,dfft(ff,xx)-dfdx)\n",
    "ax[1].plot(xx,np.gradient(ff,xx)-dfdx)\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel(\"$f'(x)$\")\n",
    "ax[1].set_ylabel('Residual')\n",
    "ax[0].legend(ncol=3,fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c223eb",
   "metadata": {},
   "source": [
    "This spectral approach even works for stiff functions that finite differences struggle with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924dc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a stiffer example:\n",
    "ff = np.cos(2*np.pi*xx)/(1. + 0.999*np.sin(2*np.pi*xx))\n",
    "dfdx =-2*np.pi*np.sin(2*np.pi*xx)/(1. + 0.999*np.sin(2*np.pi*xx)) \\\n",
    "      -np.cos(2*np.pi*xx)/(1. + 0.999*np.sin(2*np.pi*xx))**2*2*np.pi*0.999*np.cos(2*np.pi*xx)\n",
    "\n",
    "f,ax = plt.subplots(2,1,figsize=(12,9),sharex=True)\n",
    "ax[0].plot(xx,dfft(ff,xx),label='fft')\n",
    "ax[0].plot(xx,np.gradient(ff,xx),label='np.gradient')\n",
    "ax[0].plot(xx,dfdx,label='truth')\n",
    "ax[1].plot(xx,dfft(ff,xx)-dfdx)\n",
    "ax[1].plot(xx,np.gradient(ff,xx)-dfdx)\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel(\"$f'(x)$\")\n",
    "ax[1].set_ylabel('Residual')\n",
    "ax[1].set_yscale('symlog',linthresh=1e-4)\n",
    "ax[0].legend(ncol=3,fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd9ee7",
   "metadata": {},
   "source": [
    "However, it fails dramatically when the function being differentiated does not satisfy the same boundary conditions as the basis Fourier modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a non-periodic function:\n",
    "ff = np.cos(20*xx)*np.exp(-2*xx)\n",
    "dfdx = -20*np.sin(20*xx)*np.exp(-2*xx) - 2*np.cos(20*xx)*np.exp(-2*xx)\n",
    "\n",
    "f,ax = plt.subplots(2,1,figsize=(12,9),sharex=True)\n",
    "ax[0].plot(xx,dfft(ff,xx),label='fft')\n",
    "ax[0].plot(xx,np.gradient(ff,xx),label='np.gradient')\n",
    "ax[0].plot(xx,dfdx,label='truth')\n",
    "ax[1].plot(xx,dfft(ff,xx)-dfdx)\n",
    "ax[1].plot(xx,np.gradient(ff,xx)-dfdx)\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel(\"$f'(x)$\")\n",
    "ax[1].set_ylabel('Residual')\n",
    "ax[0].legend(ncol=3,fontsize=14)\n",
    "plt.subplots_adjust(hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3aed6",
   "metadata": {},
   "source": [
    "The last case fails because we are attempting to expand the sampled function in a basis that does not satisfy the same boundary conditions (i.e., $f$ is not periodic). This is equivalent to power ``leakage'' that occurs when Fourier-transforming a signal over a sub-domain on which it is not periodic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ad7eb",
   "metadata": {},
   "source": [
    "## Root finding\n",
    "\n",
    "We often need to find the zeros (roots) of a function, i.e. the values $x$ where $f(x)=0$. One approach to this is called Newton-Raphson iteration. Like everything else we've done in this section, Newton's method starts with a Taylor expansion of $f$ around an arbitrary $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x+\\delta)\n",
    "    =f(x) + f'(x)\\delta +...+\\mathcal{O}(\\delta^2),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\delta$ is some small parameter. The goal is to find $\\delta$ such that $f(x+\\delta)$ tends to zero. Ignoring quadratic and higher powers of $\\delta$ and setting $f(x+\\delta)=0$, the Taylor expansion above leads to $\\delta = -f(x)/f'(x)$. This suggests an iterative approach: at a given $x_i$, compute the next step toward a root from\n",
    "\n",
    "$$x_{i+1}=x_i+\\delta_i=x_i-\\frac{f(x_i)}{f'(x_i)}.$$\n",
    "\n",
    "Note that if your function has multiple roots, the one returned by the Newton iteration above will depend on the initial x0 provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dbe43",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Your homework asks you to write a function that performs a Newton iteration on a provided function. Parts of the session later on today will depend on your understanding of this approach, so ask me any questions you may have about that problem now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41d8f1",
   "metadata": {},
   "source": [
    "## Scipy options\n",
    "\n",
    "The library scipy.optimize has a few other functions for root-finding with a bit more functionality (in particular scipy.optimize.newton and scipy.optimize.brentq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
