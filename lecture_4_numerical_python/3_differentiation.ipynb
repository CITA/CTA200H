{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3637973c",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "This section focuses on ways to numerically approximate the derivative of a function $f(x)$ with respect to an independent variable $x$. If we know $f$ explicitly we can use symbolic manipulations find $f'=\\frac{\\text{d}f}{\\text{d}x}$. Let's say that instead we only have data $f(x_n)$ sampled at $N$ points $x_0,x_1,...,x_n,...,x_{N-1}$. Taylor expanding $f$ at a point $x_n+h$ close to a given node $x_n$,\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x_n+h)=f(x_n) + hf'(x_n) + \\frac{1}{2}h^2f''(x_n) + \\mathcal{O}(h^3)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Taking $h$ to be the separation between successive grid points, ``finite difference'' methods use taylor expansions to approximate derivatives with an error that scales with some power $h$. For example, the first order forward finite difference approximation writes\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{h}[f(x_n+h)-f(x_n)] + \\mathcal{O}(h).\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, we might write\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{h}[f(x_n)-f(x_n-h)] + \\mathcal{O}(h).\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, both forward and backward finite differences perform poorly. A better approach is to combine the Taylor expansions for $f(x_n+h)$ and $f(x_n-h)$ to produce the ``centred'' finite difference approximation\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x_n)=\\frac{1}{2h}[f(x_n+h)-f(x_n-h)] +  \\mathcal{O}(h^2).\n",
    "\\end{equation}\n",
    "\n",
    "Of course we can't apply this formula to the endpoints $x_0$ and $x_{N-1}$ of the grid, since there we don't have $x_n-h$ and $x_n+h$ (respectively). The following function applies this centred differences approximation on interior points, and forward/backward approximations on the endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace516dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fundiff(ff,xx):\n",
    "    '''\n",
    "        computes the second-order finite differences approximation to the derivative of a \n",
    "        function f with values sampled at a grid-points xx, assuming that the grid-points\n",
    "        in xx are evenly spaced\n",
    "    '''\n",
    "    # compute array of grid-spacing h, and check that xx are all evenly spaced:\n",
    "    hh = xx[1] - xx[0]\n",
    "    if np.any(abs(np.diff(xx) - hh)>1e-14):\n",
    "        raise ValueError('expected evenly spaced grid')\n",
    "    \n",
    "    # initialize array to hold derivative values:\n",
    "    dfdx = np.zeros_like(ff)\n",
    "    \n",
    "    # use centred differences to compute derivatives at interior points:\n",
    "    dfdx[1:-1] = (ff[2:] - ff[:-2])/hh/2.\n",
    "    \n",
    "    # apply forward/backward formulae at endpoints:\n",
    "    dfdx[0] = (ff[1] - ff[0])/hh\n",
    "    dfdx[-1]= (ff[-1]- ff[-2])/hh\n",
    "    return dfdx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e002514",
   "metadata": {},
   "source": [
    "Because this function applies a less accurate finite difference approximation at the boundaries, we expect the error to be larger there. The numpy function np.gradient uses much the same approach as above, but with some more bells and whistles (differentiation along one axis of an array, higher order approximations at boundaries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0097037",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0,1,100)\n",
    "ff = np.cos(20*xx)*np.exp(-2*xx)\n",
    "dfdx = -20*np.sin(20*xx)*np.exp(-2*xx) - 2*np.cos(20*xx)*np.exp(-2*xx)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(xx,fundiff(ff,xx),label='fundiff',lw=5)\n",
    "plt.plot(xx,np.gradient(ff,xx),label='np.gradient',lw=5)\n",
    "plt.plot(xx,np.gradient(ff,xx,edge_order=2),label='np.gradient (2nd order edges)',lw=5)\n",
    "plt.plot(xx,dfdx,label='truth',lw=5)\n",
    "plt.xlabel('$x$',fontsize=16)\n",
    "plt.ylabel(\"$f'(x)$\",fontsize=16)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0950e",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The second-order, forward, centred, and backward finite difference approximations for the \\emph{second} derivatives of a function are\n",
    "\\begin{align}\n",
    "    f''(x)\n",
    "    &\\simeq\\frac{1}{h^2}[f(x + 2h) -2 f(x+h) + f(x)] \\hspace{2em} \\text{(forward)},\n",
    "\\\\\n",
    "    &\\simeq\\frac{1}{h^2}[f(x + h) -2f(x) + f(x - h)] \\hspace{2em} \\text{(centred)},\n",
    "\\\\\n",
    "    &\\simeq\\frac{1}{h^2}[f(x) - 2 f(x -h) + f(x - 2h)] \\hspace{2em} \\text{(backward)}.\n",
    "\\end{align}\n",
    "Write a function that computes the second derivative of a function sampled at grid-points with a user-supplied constant separation h. Test it on the function considered above, and compare with the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0808a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
